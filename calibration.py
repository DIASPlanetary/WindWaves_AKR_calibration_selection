#!usr/bin/env python3
"""
18/02/2021

Program to contain all relevant Wind/Waves calibration
"""
import argparse
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from spacepy import pycdf
from scipy.io.idl import readsav
import scipy.interpolate as interpolate


def load_cdf(filepath):
    """
    Specifically for use with Wind pipeline, namely calibration.
    Using GSE latitude for scaling factor and radial distance for normalisation
    """
    if type(filepath) is not str:
        cdf = pycdf.CDF(str(filepath))
    else:
        cdf = pycdf.CDF(filepath)

    return cdf


def interpolate_ephemeris(eph_df, datetime_label, ephemeris_resolution, interpolate_resolution,
    eph_cols):
    """
    Interplates original ephemeris data in `eph_df` at `ephemeris_resolution` to
    `interpolate_resolution`, both given in units of minutes.
    
    Uses `datetime_label` to
    identify the datetimes over which to interpolate 

    Set to interpolate all GSE coordinate axes, radial distance and latitude
    """

    # using integer index as 'x-value' over which to interpolate
    #   NB can't convert directly back to datetime... unless use `interpolate_resolution`
    eph_time = np.arange(0, len(eph_df.index), 1)
    eph_dt = eph_df[datetime_label]

    # number of new points given current and new resolution in minutes
    n_new_points = int(eph_time.shape[0] * (ephemeris_resolution / interpolate_resolution))

    # new, finer resolution time grid
    fine_eph_time = np.linspace(eph_time.min(), eph_time.max(),
        n_new_points)

    fine_data_arr = np.zeros((len(eph_cols), n_new_points))
    # now interpolate bounded orbit datetimes, given 12 minute resolution
    for i, old_c in enumerate(eph_cols):

        # eg interpolation
        fine_func = interpolate.interp1d(eph_time, eph_df[old_c])
        # fine_func = interpolate.interp1d(eph_time, eph_df[old_c], 'cubic')

        fine_data = fine_func(fine_eph_time)

        fine_data_arr[i, :] = fine_data

    df_dict = dict()

    for i, col in enumerate(eph_cols):
        df_dict[col] = fine_data_arr[i, :]

    # create new time array to append to interpolated data
    fine_eph_dt = pd.date_range(eph_dt.min(), eph_dt.max(),
        periods=n_new_points)

    df_dict[datetime_label] = fine_eph_dt

    # create all columns - then create new df with index
    new_df = pd.DataFrame(df_dict)

    return new_df


def add_position(source_df, source_dt_label,
    target_df, target_dt_label, source_data=['RADIUS']):

    def unix_time(df, index=False,
        dt_label='datetime_ut'):
        'For converting datetimes into Unix time for interpolation and eg adding position data'
        new_df = df
        if not index:
            new_df['unix'] = new_df[dt_label].astype(np.int64) / 1e9
        else:
            new_df['unix'] = new_df.index.astype(np.int64) / 1e9

        return new_df

    if 'unix' not in source_df.columns:

        source_df = unix_time(source_df, dt_label=source_dt_label)

    target_df = unix_time(target_df, dt_label=target_dt_label)

    print_minmax = lambda df: print(df['unix'].min(), df['unix'].max())
    print_minmax(source_df)
    print_minmax(target_df)

    for label in source_data:

        func = interpolate.interp1d(source_df['unix'], source_df[label])

        target_df[label] = func(target_df['unix'])

    return target_df



def gain_factor(gamma, length):
    'Calculate G, given Gamma ratio and antenna length in m'

    z0 = 376.730313668

    gain = (gamma**2) * (length**2) * z0

    return gain


def z_flux(z_power, gain):
    """
    Calibrate Z antenna power, omitting term due to Wind latitude
    (ie for Type IIIs - still assumption of point source implicit
    here)
    """

    return np.divide(2. * z_power, gain)


def z_akr_flux(z_power, latitude, gain):
    """
    Given the power spectral density received by the Z antenna and the
    spacecraft GSE latitude, return the associated flux, assuming a point
    radio source located at Earth's center and with zero linear polarisation
    (following simplifications to MF80)

    Assumes z_power given in V^2Hz^-1
    """

    return np.divide(2. * z_power, gain * np.power(np.cos(latitude), 2))


def calibrate_z(power, latitude):
    """
    Perform calibration on the Z antenna measurements using equation 3
    of Waters+2021

    Takes:
        Z power in V^2.Hz^-1
        GSE latitude in radians

    Should then give flux in W.m^2.Hz^-1 as per equation 1 of Zarka 2004
    """

    gamma = 0.4
    length_z = 4.65 # m - quoted in Wind Docs (paragraph before ยง1.4.1.2)

    # new value using Z antenna instrument values and Zarka+2004
    gain = gain_factor(gamma, length_z)

    flux = z_akr_flux(power, latitude, gain)

    return flux


def calibration(spin_df, filepath, dt_label='DATETIME_Z'):
    """
    Given a Wind ephemeris CDF file from NASA SPDF (see download
    instructions in README), stored at `filepath`, load and interpolate relevant fields
    (GSE latitude and radial distance) to 45s resolution and append ephemeris data to
    L2 data in `spin_df`. 
    """
    
    cdf = load_cdf(filepath) # load CDF file from given filepath

    # check for appropriate ephemeris data (using default data labels from SPDF)
    if ('GSE_LAT' in cdf.keys()) and ('RADIUS' in cdf.keys()):

        df_dict = {
            'Epoch': np.array(pd.Timestamp(d) for d in cdf['Epoch'][:]),
            'GSE_LAT': np.array(cdf['GSE_LAT'][:]),
            'RADIUS': np.array(cdf['RADIUS'][:])
        }

        df = pd.DataFrame(df_dict)
        print(df.head())

    else:

        raise ValueError("Required ephemeris data not found in CDF. GSE latitude and radial distance required.")
    
    # include check for appropriate date range here
    # get min/max dates for L2 data and ephemeris
    min_l2_date, max_l2_date = spin_df['DATETIME_Z'].min(), spin_df['DATETIME_Z'].max()
    min_eph_date, max_eph_date = df['Epoch'].min(), df['Epoch'].max()
    # perform check, raising error if inappropriate date range
    if (min_eph_date > min_l2_date) or (max_eph_date < max_l2_date):
        raise ValueError("Ephemeris CDF doesn't cover appropriate time range. Ensure ephemeris covers period from {} to {}.\nEphemeris currently covers {} to {}".format(
            min_l2_date.strftime('%d %b %Y (DOY %j) %H:%M:%S'), max_l2_date.strftime('%d %b %Y (DOY %j) %H:%M:%S'),
            min_eph_date.strftime('%d %b %Y (DOY %j) %H:%M:%S'), max_eph_date.strftime('%d %b %Y (DOY %j) %H:%M:%S')))

    # now need to interpolate data and ensure works with calibration.calibration    
    eph_df = interpolate_ephemeris(df, 'Epoch', 12, 0.75, ['GSE_LAT', 'RADIUS'])

    spin_df = add_position(eph_df, 'Epoch',
        spin_df, 'DATETIME_Z', ['GSE_LAT', 'RADIUS'])

    power = spin_df['AMPL_Z'] * 1.e-12
    lat = np.radians(spin_df['GSE_LAT'])

    spin_df['flux_si'] = calibrate_z(power, lat)

    return spin_df


def contiguous_sweep_datetimes(l3_df, sweep_label, dt_label,
    sweep_period=183, return_original=False):
    """
    Return appropriate datetime labels for each 3-minute spectrum of L2 data,
    as well as for non-existent sweeps to maintain contiguity (as with
    spectrograms)

    taken from spectrogram_plotting.spectrogram_array
    """

    # Get list of dates of recorded sweep cycles
    dates = np.array([pd.Timestamp(np.min(df[dt_label].values))
        for _, df in l3_df.groupby(sweep_label)])
    
    # get ideal number of sweeps for the period in the data, given sweep_period
    # in seconds
    n_ideal_sw = np.ceil((dates[-1] - dates[0]).total_seconds() / sweep_period)

    dtimes = pd.date_range(dates[0], dates[-1], periods=n_ideal_sw)

    if not return_original:
        return dtimes
    else:
        return dates, dtimes


def ft_array(csv_df, param_label, set_zero_nan=True, zero=True):
    """
    Create 2D array corresponding to frequency-time spectrogram of `param_label`

    `sweep_period` in seconds
    """
    sweep_label = 'SWEEP'
    freq_label = 'FREQ'
    dt_label = 'DATETIME_Z'
    # wind sweep period in seconds
    sweep_period = 183

    sweep_dates, ideal_dates = contiguous_sweep_datetimes(csv_df, sweep_label, dt_label,
        return_original=True)
    # print(sweep_dates.shape)

    # Store indices for sweeps that correspond to datetime position in array
    sweep_array_inds = np.array([pd.Index(ideal_dates).get_loc(d, method='nearest')
        for d in sweep_dates])
    # print(sweep_array_inds.shape)

    freqs = np.sort(csv_df[freq_label].dropna().unique())
    
    # array for sampled frequency range and sweep cycles
    if zero:
        out_array = np.zeros((freqs.shape[0], int(ideal_dates.shape[0])))
    else:
        out_array = np.ones((freqs.shape[0], int(ideal_dates.shape[0])))
    # Filling array with sampled frequency means
    for arr_i, (_, sweep_df) in zip(sweep_array_inds,
        csv_df.groupby(sweep_label)):

        # print(arr_i, sweep_df['SWEEP'].unique())


        # groupby to average parameters
        parameter_mean = sweep_df.groupby(freq_label).agg({param_label: np.nanmean})

        # apparently no need to manually flip - first output shows
        # upside-down spectra
        means = parameter_mean[param_label].values
        # if np.all(sweep_df['sweep_flag'] == 1):
        #     print(means.shape)

        # assuming array sizes less than n_freqs have missing vales/erroneous
        # sweeps, so set to NaN
        if len(means) < freqs.shape[0]:
            means = np.repeat(np.nan, freqs.shape[0])
        
        # out_array[:, int(sw_i)] = flipped_means
        out_array[:, arr_i] = means

        # if np.all(sweep_df['sweep_flag'] == 1):
        #     print(sweep_df.loc[:, ['SWEEP', 'sigma_z']])
        #     print(sweep_df.shape)

    
    if set_zero_nan:
        # prevent division by 0 in colorbar
        out_array = np.where(np.isclose(out_array, 0., atol=1e-31), np.nan, out_array)
    # print(np.min(out_array))
    # print(np.nanmin(out_array))
    # print(np.max(out_array))
    # print(np.nanmax(out_array))

    return out_array



def create_calibrated_flux_dataframe(data,
    flux_label='flux_si', out_flux_label='flux_si', sweep_label='SWEEP'):
    """
    Mostly identical to create_selected... but for calibrated data only

    Takes 3 minute arrays of SNR and sigma_z also
    """
    # NEW 14TH MARCH 
    # (setting freqs only as those from non-flagged data made arrays
    # different lengths - error on out_df creation)
    # data = data.loc[data['sweep_flag'] == 0, :]

    flux = ft_array(data, flux_label)
    snr = ft_array(data, 'SNR_dB', set_zero_nan=False)
    sigma_z = ft_array(data, 'sigma_z', set_zero_nan=False)

    sweep_flag = ft_array(data, 'sweep_flag', set_zero_nan=False, zero=False)

    real_dtimes, out_dtimes = contiguous_sweep_datetimes(data,
        'SWEEP', 'DATETIME_Z', return_original=True)
    freqs = np.sort(data['FREQ'].unique())
    # NEW 11TH MARCH - PREVENT WEIRD FREQUENCIES?
    # freqs = np.sort(data.loc[data['sweep_flag'] == 0, 'FREQ'].unique())

    series_length = flux.shape[0] * flux.shape[1]
    print(series_length) 

    # tile frequencies for each spectrum
    n_spectra = flux.shape[1]

    # get appropriate list of frequencies for output dataframe
    df_freqs = np.array(np.tile(freqs, n_spectra), dtype=np.float64)
    print(df_freqs.shape[0])
    n_freqs = flux.shape[0]

    # flatten all 3-minute ft arrays
    flux = pd.Series(flux.flatten(order='F'))

    snr = pd.Series(snr.flatten(order='F'))

    sigma_z = pd.Series(sigma_z.flatten(order='F'))

    sweep_flag = pd.Series(sweep_flag.flatten(order='F'))

    # get indexes for each spectrum, assigned to data from each frequency
    spectrum_i = np.repeat(np.arange(n_spectra), n_freqs)
    # 15th March - so not to overlap with bad sweep nos from L2 data
    # spectrum_i = np.repeat(data['SWEEP'].unique(), n_freqs)
    # 15th March - using original, as no problem with overlap if don't concat bad_dfs
    # (Now using the empty spectra of ft_array output to infer bad sweeps...)

    # dtimes = np.repeat(dtimes, n_freqs) # old - `dtimes` was the list of "fake" dts
    # dtimes = np.repeat(real_dtimes, n_freqs) # 15 March - actual min sweep times of "good" sweeps
    dtimes = np.repeat(out_dtimes, n_freqs) # 15 March - fake times of all sweeps - including empty spectra created in ft_array calls

    # if data['sweep_flag'].unique().shape[0] > 1:
    #     raise ValueError("Multiple values for sweep flag")
    # else:
        # sweep_flag = data['sweep_flag'].unique()[0]
        

    df_dict = {
        'freq': df_freqs,
        'datetime_ut': dtimes,
        'spectrum_i': spectrum_i,
        'snr_db': snr,
        'sigma_z': sigma_z,
        out_flux_label: flux,
        'sweep_flag': sweep_flag}

    out_df = pd.DataFrame(df_dict)

    return out_df


def apply_crosscal_spectrum(cal_df):
    """
    16/4/21

    Load and apply the scaling spectrum derived from comparing Type III bursts
    etc

    """
    fp = 'cross_calibration_scaling_spectrum.csv'
    
    # load scaling spectrum, setting frequency as index
    df = pd.read_csv(fp, index_col=0)

    cal_df['new_flux'] = [flux / df.loc[f, 'avg_offset']
        for f, flux in zip(cal_df['freq'], cal_df['flux_si'])]
    
    cal_df = cal_df.drop(columns=['flux_si'])
    cal_df = cal_df.rename(columns={'new_flux': 'flux_si'})

    return cal_df
